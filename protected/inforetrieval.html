<!doctype html>
<html lang="en">

<head>
        <script src="http://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
        <script src="scripts/melissalafranchise.js"></script>

        <link href='http://fonts.googleapis.com/css?family=Alegreya+Sans+SC:100,500' rel='stylesheet' type='text/css'>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width"> <!--to override zoom-out of mobile browsers-->
        <meta name="author" content="Melissa Lafranchise">
        <meta name="description" content="Competency 5: Design, query and evaluate information retrieval systems.">

        <title>5 Information Retrieval</title>

        <link rel="stylesheet" href="styles/style.css">

        <!--[if lt IE 9]>
        <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body class="competencies">

        <header>

                <h1><a href="../index.html" title="Link to homepage" class=homelink>melissa lafranchise &nbsp; &bull; &nbsp; eportfolio</a></h1>

        </header>

        <div class="main">

                <nav id="accordmenu"> <!--Navigation based on Flat JQuery Accordian Menu by Russell Martin. http://cssmenumaker.com/blog/flat-jquery-accordion-menu-tutorial-->
                        <ul>
                           <li><a href="intro.html">Introduction</a></li>
                           <li><a href="phil.html">Professional Philosophy</a></li>
                           <li class="competencieslink"><a href="#">Core Competencies <span id="minus">&#43;</span></a>
                              <ul>
                                <li><a href="ethics.html">1 Ethics, Values &amp; Principles</a></li>
                                <li><a href="settings.html">2 Organizational Settings</a></li>
                                <li><a href="diversity.html">3 Recognize Diversity</a></li>
                                <li><a href="planning.html">4 Planning &amp; Management</a></li>
                                <li class="inforetrievallink"><a href="inforetrieval.html">5 Information Retrieval</a></li>
                                <li><a href="collection.html">6 Collection Management</a></li>
                                <li><a href="organizing.html">7 Organizing Information</a></li>
                                <li><a href="technology.html">8 Communication Technologies</a></li>
                                <li><a href="access.html">9 Information Access</a></li>
                                <li><a href="infoseeking.html">10 Info-Seeking Behavior</a></li>
                                <li><a href="learningtheories.html">11 Learning Theories</a></li>
                                <li><a href="research.html">12 Research Methods</a></li>
                                <li><a href="communication.html">13 Communication Skills</a></li>
                                <li><a href="programs.html">14 Programs &amp; Services</a></li>
                              </ul>
                           </li>
                           <li><a href="conclusion.html">Conclusion</a></li>
                           <li><a href="affirmation.html">Affirmation</a></li>
                        </ul>

                </nav>

               <div class="content">

                        <h2>5. Information Retrieval</h2>

                        <p>Information retrieval is fundamental to information science and to the service-oriented organizations of libraries, archives, and museums. These institutions house information (e.g., books, records, art, and artifacts) for the express purpose of sharing it with an audience. To share information in an efficient, effective, and meaningful way, the information must be organized appropriately for retrieval. For the user of these services, information retrieval is the act of finding information one needs to serve some purpose in his/her life (Meadows et al., 2007; Rowley & Hartley, 2008). It also implies that those who organize the knowledge/information must do so in a way that makes information retrieval possible through a variety of people-based and computer-based tools (Rowley & Hartley, 2008). Without proper study, the best methods to organize information for efficient, accurate, and relevant retrieval would be trapped in a nebulous cloud of opinion and instinct.</p>

                        <p>Effective information retrieval system design begins with an understanding of users and specifically their information-seeking behavior, as described in competency 10. Known information-seeking behavior allows the system designer to provide a simple, intuitive path for users to find the information they require. Listening to the users, understanding what they need and why they need it, allows for the creation of a system that synchs with the users’ purposes and with how they search for information (e.g., by keywords only, through browsing categories, through formal, well-defined search strategies involving Boolean operators and stemming, etc.). Morville (2005) explains this as “the vital importance of empathy for the user” and continues by saying, “only by understanding and caring about the perspective of the individual can we design useful, usable solutions” (p. 31). From a slightly different perspective, Oppenheim and Stenson (2003) also point out that information isn’t seen as significant by the user unless it can be shown to fit usefully into his/her greater purpose (p. 431). An important caveat in understanding the user is that what is relevant to any given user—even if they are part of a defined and understood user group—may vary from day to day and search to search. Information retrieval system designers must realize that it is ultimately fruitless to try to design a system that is perfect for every user at every point in time.</p>

                        <p>Whether one is searching by using keywords from natural vocabulary, using specified search syntax to employ Boolean operators to controlled vocabulary, or browsing by shelf or hyperlinked facet, the query is where the user meets and interacts with the information retrieval system. There are several defined types of queries, including sample searches where the user wants just a few, precise results; know-item searches where the user is seeking a specific piece of information they believe exists in the database; and, exhaustive searches where the user hopes to find all relevant information in the database related to a given topic (Morville, 2005).</p>

                        <p>One vital set of metrics for information retrieval system evaluation has been the duo of precision and recall. However, with the increasing scale (i.e. sheer number of documents) in today’s systems, recall can be very difficult to measure. “Precision measures how well a system retrieves only the relevant documents. Recall measures how well a system retrieves all the relevant documents” (Morville, 2005, p. 49). Both precision and recall are dependent upon the perspective of any given user, so, as I’ve mentioned so often in these competency summaries, understanding the user is fundamentally important. There are other ways to evaluate information retrieval systems, for instance, watching a user’s interaction with the system through some sort of eye tracking program or evaluating the system based on a set of best practices derived from successful systems that do meet user expectations.</p>

                        <p>Information retrieval is fundamental to any organization that has as its mission connecting people to information. This is certainly the case for libraries and archives, but it is also the case for many, many businesses that trade in both products and services. Information retrieval represents the ability of the organization to provide its users with the information they need when they need it and in a form that is comprehensible to them.</p>

                        <h3>Evidence</h3>

                        <p>Evidence that demonstrates my ability to design information retrieval systems comes from a LIBR 202 Information Retrieval descriptive metadata database project that included group and individual work. The <a href="pdfs/LIBR202_DescripMetadataDB_Group.pdf" title="Link to design example">first part of the project focused on database creation and was group based</a>. The group project’s goal was to design a database of early 21st century kitchen appliances that would be used by an audience of future anthropologists. The <a href="pdfs/LIBR202_DescripMetadataDB_Individual.pdf" title="Link to design example 2">second part of the project focused on reaction to beta testing of the database and was individual</a>. I was the team coordinator/leader for the <a href="pdfs/LIBR202_DescripMetadataDB_Group.pdf" title="Link to design example">first part of the project</a>, and, in addition to writing around 80% of the final report, I also reformatted the Power Point presentation that showcased our collection, as the team member who created it did not have much experience with Power Point, and made necessary revisions to our DB TextWorks database when the group member who created it became ill during the weekend it was due. The rest of the group work—deriving attributes, defining data structures and validation lists, writing indexing rules, and creating records for the database—was shared among all team members. As the report details, the team’s initial focus was on describing the users (i.e., future anthropologists), their information needs, and their projected use of the data contained in the database (i.e., drawing inferences about the behavior of the people who used the appliances). The next task was to derive attributes from the kitchen appliance collection that would enable future anthropologists to meet their information needs. Field types included those using controlled vocabulary from validation lists as well as open text fields that encouraged indexers to enter descriptions in natural language. The team felt these open text fields would be particularly helpful to the future anthropologists in their endeavor to understand users of 21st century kitchens. For example, the field “Interface” was an open text entry field. The team decided that an open text field was most appropriate because this attribute is key in understanding how people interact with their appliances and because that interaction was too complicated to narrow down to a list of specific values. Once the natural vocabulary for this field was gathered, it would be used to create an inverted file for key word searching. The natural vocabulary could be used to re-create the mental models associated with kitchen appliances. These models could be fed back into the database as improved search terms that draw on the concepts and statements present in the mental models (Carley & Palmquist, 1992). The indexers were also considered during the creation of indexing rules as the team wanted to provide applicable instructions, with examples, that would aid indexers in describing actual kitchen appliances. Finally, the team tested the usefulness of the attributes, data structures, and validation rules by entering test records. In each step, adjustments were made to attributes or controlled vocabulary as testing and rethinking highlighted instances of, for example, an attribute not really providing useable data, or a rule that insufficiently described the action an indexer should take. The <a href="pdfs/LIBR202_DescripMetadataDB_Individual.pdf" title="Link to design example 2">second part of the project</a> entailed reviewing the results of beta testing and gleaning a list of improvements to the database based on that feedback. This was the individual portion of the project. I created a combined quantitative and qualitative analysis of the feedback to derive the list of improvements. Beta testing is vital in the design stage of information retrieval systems because this is where objective testers determine if the database structures function correctly and if the database will be able to contain the information that it was designed to store. I began with the quantitative summary of feedback from the beta tester survey that each tester completed and compiled the data into charts that clearly visualized the database’s success or failure in meeting basic evaluative criteria. (To clarify, the survey itself was designed by the professor, not by me.) I then addressed each criterion on which the database was evaluated (i.e., statement of purpose, data structures, indexing rules) and commented on the specific feedback of each beta tester, especially where the feedback highlighted concerns or confusion. From this analysis, I concluded that the team had accomplished its goal of creating an effective descriptive metadata database, but there was room for improvement. The next section of the report specified 13 recommendations for improvements derived from the feedback. What followed was a discussion of information retrieval concepts specifically related to, among other things, the importance of end-user focus when designing a database and the value of beta testers in improving that design. I think that both the group and individual portions of this project were well executed, and I have no major changes to them today. (Incidentally, the professor submitted both the group report and my individual report as examples of quality student work for the SJSU SLIS ALA accreditation report.)</p>

                        <p>Evidence of my ability to query information retrieval systems is provided by a portion of my <a href="pdfs/LIBR256_ReferenceSourcesEval.pdf" title="Link to query example">LIBR 256 Archives and Manuscripts resource evaluation</a> and by <a href="pdfs/LIBR285_Discussion_Query.pdf" title="Link to query example 2">a discussion post for LIBR 285 Research Methods in Information Science</a>. The <a href="pdfs/LIBR256_ReferenceSourcesEval.pdf" title="Link to query example">LIBR 256 report</a> compared six online archival collections on search performance and interface usability, hence querying each of the six repositories was required. In these queries, I mimicked the behavior of a novice searcher who would employ simple, keyword searches. Two searches were completed for each database: a known-item search (i.e., using keywords “Walt Whitman”) and a sample search on a general topic (i.e., using keywords “19th century poetry”). Each search interface for the six resources was quite different and required interpreting how best to enter the search queries. Precision scores based on relevant results out of all results were calculated and compared for each information retrieval system with quite varying results. In fact one resource, The Repository of Primary Sources, only allowed browsing, no search function was available. Appendix A Detailed Search Performance by Reference Resource includes the detailed results of these queries. The <a href="pdfs/LIBR285_Discussion_Query.pdf" title="Link to query example 2">LIBR 285 discussion post</a> outlined a search strategy for a possible topic for my research proposal as well as a list of reference resources that I would search. While my actual research topic ultimately evolved into a rather different entity, this post still highlights my ability to develop search queries to put to information retrieval systems, including concept definition, the use of Boolean operators, stemming, and the use of concept sets. One final note, search engine and King Library queries were performed for almost every course and assignment, though I don’t have a particular piece of evidence that shows specific queries and their results. Looking back on the LIBR 256 evaluation today, I would add some more advanced search queries to the evaluation to truly test the information retrieval systems’ functionality. The LIBR 285 search queries still seem sound to me today.</p>

                <p>My ability to evaluate information retrieval systems is demonstrated in <a href="pdfs/LIBR202_A3_SystemEvaluation.pdf" title="Link to system evaluation">a system evaluation produced for LIBR 202 Information Retrieval</a>. The evaluation was completed for the Multnomah County Library OPAC and website. After a summary of the library and website background, some suggestions regarding collection scale, and speculation as to possible indexing mechanisms, I then investigated search features and functions as well as retrieval performance. The search features and functions were detailed under competency 10 Information-Seeking Behavior. In summary, I evaluated the OPAC against a set of 12 heuristics gleaned from two research reports that investigated user information-seeking behavior and expectations in today’s web-savvy environment. Overall, the Multnomah County Library site performed well exhibiting 79% of the best practices. For example, one best practice was a metasearch of all library assets through one search field. The Multnomah County Library’s metasearch of the entire site was a superb feature which not only searched the OPAC, but also searched the library’s website content. It did this all from an intuitive (since the advent of search engines like Google), single search box located prominently on the homepage. Following this, I delved into an evaluation of search performance. I queried the system using simple keyword searches, specific facet searches, and advanced searches to emulate what might be expected of several typical library users. All three types of searches were repeated for a known-item and a sample search. Effectiveness of each search was determined by calculating the precision score. Overall, the simple specific-facet search was most successful followed by the simple keyword and advanced search. I included several charts to visually represent more detailed precision results (see pages 11-13). Recall was also calculated though it was difficult to truly understand the total number of relevant documents included in the entire system. Hence, I could only estimate a total number of relevant documents in the database based on the highest total results returned across the three search types. Overall recall was highest for the advanced search followed by the simple keyword search and then the specific facet search. I only included a single chart to visually represent the recall data (see page 13) as the calculation was based on an estimation rather than a known value, hence I considered the data as rather suspect. The detailed results of the searches can be found in Appendix C Search Results Data. I followed the quantitative analysis with a brief qualitative assessment of the interface design. Ultimately I conclude that the Multnomah County Library OPAC is a successful information retrieval system—going so far as to suggest that it be considered a model for other public libraries’ OPACs and websites. I stand by this evaluation today.</p>

                        <p>The group and individual descriptive metadata database assignments from LIBR 202, the resource evaluation and its queries from LIBR 256, the well-defined search query for LIBR 285, and the detailed OPAC and website evaluation completed for LIBR 202 illustrate my ability to design, query, and evaluate information retrieval systems with an understanding of their vital importance to the core mission of any information organization—connecting users to the information they seek.</p>

                        <h3>References</h3>

                        <p class="hangingindent">Carley, K., & Palmquist, M. (1992). Extracting, representing and analyzing mental models. <em>Social Forces, 70</em>(3), 601-636. Retrieved from <a href="http://sf.oxfordjournals.org/">http://sf.oxfordjournals.org/</a></p>

                        <p class="hangingindent">Meadow, C. T., Boyce, B. R., Kraft, D. H., & Barry, C. (2007).  <em>Text information retrieval systems</em> (3rd ed.). San Diego, CA: Academic Press.</p>

                        <p class="hangingindent">Morville, P. (2005). <em>Ambient findability</em>. Sebastopol, CA: O’Reilly.</p>

                        <p class="hangingindent">Oppenheim, C. & Stenson, J. (2003). Studies on information as an asset II: repertory grid. <em>Journal of Information Science, 29</em>(5), 419-432. Retrieved from <a href="http://jis.sagepub.com/">http://jis.sagepub.com/</a></p>

                        <p class="hangingindent">Rowley, J., & Hartley, R. (2008). <em>Organizing knowledge: An introduction to managing access to information</em> (4th ed.). Burlington, VT: Ashgate Publishing Limited.</a></p>

               </div> <!--End div content-->

               <aside class="sidebar">

                        <h3>Evidence</h3>

                        <p><a href="pdfs/LIBR202_DescripMetadataDB_Group.pdf" title="Link to design example">LIBR 202 group project report</a></p>
                        <p><a href="pdfs/LIBR202_DescripMetadataDB_Individual.pdf" title="Link to design example 2">LIBR 202 individual report</a></p>
                        <p><a href="pdfs/LIBR256_ReferenceSourcesEval.pdf" title="Link to query example">LIBR 256 reference resource evaluation</a></p>
                        <p><a href="pdfs/LIBR285_Discussion_Query.pdf" title="Link to query example 2">LIBR 285 discussion search query</a></p>

                        <p><a href="pdfs/LIBR202_A3_SystemEvaluation.pdf" title="Link to system evaluation">LIBR 202 system evaluation</a></p>
               </aside>

        </div> <!--End div main-->

       <footer>

                <p>&copy;2014 Melissa Lafranchise  &bull;  Seattle, WA  &bull;  <a href="mailto:pulchrit@gmail.com" title="Email Melissa">pulchrit@gmail.com</a></p>

       </footer>
</body>

</html>
