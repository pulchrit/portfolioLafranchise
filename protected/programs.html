<!doctype html>
<html lang="en">

<head>
        <script src="http://code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
        <script src="scripts/melissalafranchise.js"></script>

        <link href='http://fonts.googleapis.com/css?family=Alegreya+Sans+SC:100,500' rel='stylesheet' type='text/css'>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width"> <!--to override zoom-out of mobile browsers-->
        <meta name="author" content="Melissa Lafranchise">
        <meta name="description" content="Competency 14: Evaluate programs and services based on measurable criteria.">

        <title>14 Evaluate Programs and Services</title>

        <link rel="stylesheet" href="styles/style.css">

        <!--[if lt IE 9]>
        <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body class="competencies">

        <header>

                <h1><a href="../index.html" title="Link to homepage" class=homelink>melissa lafranchise &nbsp; &bull; &nbsp; eportfolio</a></h1>

        </header>

        <div class="main">

                <nav id="accordmenu"> <!--Navigation based on Flat JQuery Accordian Menu by Russell Martin. http://cssmenumaker.com/blog/flat-jquery-accordion-menu-tutorial-->
                        <ul>
                           <li><a href="intro.html">Introduction</a></li>
                           <li><a href="phil.html">Professional Philosophy</a></li>
                           <li class="competencieslink"><a href="#">Core Competencies <span id="minus">&#43;</span></a>
                              <ul>
                                <li><a href="ethics.html">1 Ethics, Values &amp; Principles</a></li>
                                <li><a href="settings.html">2 Organizational Settings</a></li>
                                <li><a href="diversity.html">3 Recognize Diversity</a></li>
                                <li><a href="planning.html">4 Planning &amp; Management</a></li>
                                <li><a href="inforetrieval.html">5 Information Retrieval</a></li>
                                <li><a href="collection.html">6 Collection Management</a></li>
                                <li><a href="organizing.html">7 Organizing Information</a></li>
                                <li><a href="technology.html">8 Communication Technologies</a></li>
                                <li><a href="access.html">9 Information Access</a></li>
                                <li><a href="infoseeking.html">10 Info-Seeking Behavior</a></li>
                                <li><a href="learningtheories.html">11 Learning Theories</a></li>
                                <li><a href="research.html">12 Research Methods</a></li>
                                <li><a href="communication.html">13 Communication Skills</a></li>
                                <li class="programslink"><a href="programs.html">14 Programs &amp; Services</a></li>
                              </ul>
                           </li>
                           <li><a href="conclusion.html">Conclusion</a></li>
                           <li><a href="affirmation.html">Affirmation</a></li>
                        </ul>

                </nav>

               <div class="content">

                        <h2>14. Evaluate Programs and Services</h2>

                        <p>Program and services must be evaluated to determine effectiveness. Sometimes anecdotal or qualitative value judgments are all that is required, but evaluation based on data and defined metrics, especially compared over time to show trends of success/failure, enables more simplified and codified decision-making because decisions are founded on facts that the data represents. Evans and Ward explain: “Libraries and information services face ever-increasing pressure to be accountable. . . . Without question, decisions informed with hard-data (evidence-based) carry greater creditability that those not so informed (anecdotal-based)” (2007, p. 137). There are any number of ways to gather this data, including patron surveys, usage statistics from Integrated Library Systems or website analytics tools, formal and informal research studies, attendance counting, and cost analysis. The next step is to use some sort of statistical or mathematical techniques to analyze the data. If this is the first time such data has been gathered, then it can form a baseline from which to judge future changes in success or failure rates. Otherwise, the data should be compared to older identical or sufficiently similar data to determine how successful a particular program or service has been over time or in relation to other similar programs or services.

                        <p>Not only does data allow for overall judgments of program and service success or failure, it allows for specific insights into areas or aspects of the programs and services that work or do not work. This allows LIS and other professionals to develop tactics or amend program or service aspects to specifically address, correct, or improve the problem areas, or to decide to discontinue the program or service. I also want to highlight the fact that measurable results allow professionals not only the ability to see where things went wrong and how to make improvements, but also to celebrate successes. Successful programs and services benefit patrons and staff alike. Patrons reap the intended benefit of the program or service while personnel gain job satisfaction, a sense of personal fulfillment, and increased morale.
Further, measurable evaluation criteria will illustrate accountability for the money invested not only in the particular program or service, but also in the library, information center, department or company. All professions need to justify costs, and a way to do this is show some sort of return on investment. Simply, we spent X number of dollars, but this data shows it increased library patron satisfaction by X%, or increased sales by X%, or generated XXX number of qualified leads.</p>

                        <p>To recap, the value of measuring results of programs and services to LIS professions includes the ability to judge success or failure, make improvements to or discontinue programs and services that aren’t producing desired results, celebrate successes when they occur, and justify costs of the programs and services as well as the cost of the library or information center itself.</p>

                        <p>Finally, in my experience an organization can adopt programs that have worked well at other libraries, or information center, or companies, and apply best practices in programs and services from the field or industry, but the only way to know those programs and services will work in a particular library, information center, or company is to test them out and see how the users/audience react by tracking measurable criteria. For example, at my last company, we ran Facebook ads as part of a larger advertising campaign. Per the media agency helping us with the campaign as a whole, our click-through rates on those ads were above average compared to their other clients. The reason: at the time, our audience demographics happened to coincide with the demographic where Facebook was seeing its largest audience increases (i.e., men and women over 40 years old). It was by testing the ads and tracking the results that we understood the actual effectiveness of these ads. Not all programs work for every audience. Some services must be tweaked for local needs. It is well and good to use example programs and best practices as a guide, but the true proof of effectiveness comes by evaluating actual results from data gathered.</p>


                        <h3>Evidence</h3>

                        <p>My first piece of evidence comes from my previous full time job. I was a Project Manager in the Marketing Communications department of a company that manufactured diagnostic and treatment devices for sleep apnea. The evidence is <a href="pdfs/CPAPToday_Q4Campaign_Results_ACT_8-7.pdf" title="Link to advertising campaign results">a presentation</a> given to company department heads and relating the results of a small online consumer advertising campaign. The presentation was created in tandem with the media agency that we hired to assist with the media placement plan and day-to-day maintenance of the campaign. As Project Manager, I was responsible for overseeing the entire campaign, in-house creative production of ads, website, and brochure, managing the media agency, and made all strategic decisions related to our Facebook creative, spend, and targeting (as this was a capability we wanted to keep in-house). It was also my responsibility to provide weekly updates on campaign results and this overall results presentation to stakeholders with the department and the company. This was a small ~$250k consumer advertising campaign that built off of an even smaller pilot campaign, which I also managed. Both were the first of their kind for the company, which had previously focused on trade partners instead of consumers. Both were intended to provide justification for future consumer marketing efforts. The presentation began with a re-cap of the campaign goals and parameters and specifics of the media plan. I then moved quickly into overall results. These results are provided in the form of measurable data. Hence, the number of impressions, visits to the campaign landing page, click-through rate, number of registrations through landing page for the informational brochure, and the key metric the overall cost per lead (i.e., how much did we spend to acquire each name and address of consumers interested in learning more about sleep apnea treatment) were detailed. I also compared these overall results to the previous pilot campaign results—building on those baseline numbers—illustrating increased efficiency of spend, a stable click-through rate, and an increased registration rate. The next several slides provided detailed results about impressions by media placement, audience insights, creative performance by media type, an overview of the results of a mini-awareness survey that ran in tandem with the creative campaign across the same sites. The final slides detailed what we learned, how that would be applied to future campaigns, and next steps for the consumer leads generated through this campaign. From the plethora of results measured, we were able to see which tactics worked and which didn’t and make decisions about how to craft future iterations of this consumer advertising program. I still consider this to be a decent summary of results. It and the campaign were well received by the department heads and a larger amount of money was allocated towards a subsequent consumer campaign.</p>

                        <p>My second piece of evidence also comes from my previous full time job. This <a href="pdfs/ECO_1.0Release_Results.pdf" title="Link to product launch campaign results">presentation</a> reviewed results from the marketing communications campaign of a software product launch. I again was Project Manager of the marketing communications activities of the launch, overseeing all creative development, production, execution, and tracking. I began the presentation by explaining the goal and call to action of the campaign and reviewed the tactics employed. Overall results were presented next and included number of impressions, cost per impression, number of click-throughs, the key metric of-click through rate, as well as cost per click, number of registrations, registration rate, and cost per registration. Detailed results were presented by media type with a series of charts that made it easy to see which tactics were more successful in generating clicks than others. Cost per click and per registration were also reviewed by tactic. The results of this campaign were then compared to two previous related, though not identical campaigns. Finally, I ended with a summary of successes and suggestions for improvement. This last slide showed a path forward based on what the campaign actually garnered for the company. Knowing how the campaign actually performed was due entirely to tracking measurable results. I stand by this results presentation today and feel that it illustrated both successes and failures of the campaign and enabled future campaign planning to proceed based on a real understanding of what tactics would be more effective and cost efficient.</p>

                        <p>A brief <a href="pdfs/LIBR204_CriticalSummary_ROI.pdf" title="Link to critical summary on ROI">critical summary</a> of an article written for my LIBR 204 Information Organizations and Management class represents my final piece of evidence for this competency. I include it because the article’s focus is on using ROI to establish a library’s value. Not only do the authors provide a literature review showcasing how ROI has been used by libraries in the past, they provide example formulas and links to calculators that library administrators or department heads can implement to use the data they have collected, illustrating the worth of their programs and services in a quantified way. Importantly, the authors recognize that ROI is best used with specific audiences, that is funding agencies and those concerned with the bottom line in terms of dollars spent and results derived. Equally, they spend a significant amount of time demonstrating ways in which libraries can assign a value to programs and services that are typically free. This is so helpful to library professionals whose altruistic efforts to provide access to information come up against local, state, and federal governments’ need to cut budgets. It gives them a vocabulary with which they can speak directly to these decision makers. Though very brief, I believe my critical summary effectively highlights the key points in this article regarding the reason to measure results and specific ways to do so through ROI, and I would make no changes today.</p>

                        <p></p>Both results presentations from my previous full time job and the brief article critique written for LIBR 204 all serve to illustrate my understanding of the value of evaluating programs and services through measurable criteria. In the first two instances, significant investments were made and the justification for those expenses was demonstrated in a quantified number of leads for the company as well as knowledge of how to successfully plan future campaigns. In the third instance, library administrators were shown how to incorporate a business concept into their efforts to prove the value of their programs and services and secure future funding for the library.</p>

                        <h3>References</h3>

                        <p class="hangingindent">Evans, G. E., & Ward, P. L. (2007). <em>Management basics for information professionals</em>. New York, NY: Neal-Schuman Publishers.</p>


               </div> <!--End div content-->

               <aside class="sidebar">

                        <h3>Evidence</h3>

                        <p><a href="pdfs/CPAPToday_Q4Campaign_Results_ACT_8-7.pdf" title="Link to advertising campaign results">Consumer advertising campaign results</a></p>

                        <p><a href="pdfs/ECO_1.0Release_Results.pdf" title="Link to product launch campaign results">Product launch campaign results</a></p>

                        <p><a href="pdfs/LIBR204_CriticalSummary_ROI.pdf" title="Link to critical summary on ROI">LIBR 204 Critical summary of article on ROI</a></p>

               </aside>

        </div> <!--End div main-->

       <footer>

                <p>&copy;2014 Melissa Lafranchise  &bull;  Seattle, WA  &bull;  <a href="mailto:pulchrit@gmail.com" title="Email Melissa">pulchrit@gmail.com</a></p>

       </footer>
</body>

</html>
